{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inpired by this LlamaIndex notebook:\n",
    "\n",
    "https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/coa_agent.ipynb\n",
    "\n",
    "Making some changes to it with the only intention of trying ideas and learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that I am assuming you have the relevant API_KEYs as environmental variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-packs-agents-coa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.agent import AgentRunner\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "import os\n",
    "import subprocess\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.packs.agents_coa import CoAAgentWorker\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\", embed_batch_size=256\n",
    ")\n",
    "Settings.llm = OpenAI(model=\"gpt-4-turbo\", temperature=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANIES = [\"uber\", \"lyft\"]\n",
    "DATA_DIRS = {}\n",
    "PERSIST_DIRS = {}\n",
    "for c in COMPANIES:\n",
    "    DATA_DIRS[c] = os.path.join(os.environ[\"DATA_DIR\"], f\"{c}\")\n",
    "    PERSIST_DIRS[c] = os.path.join(os.environ[\"PERSIST_DIR\"], f\"{c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "You can access more files by usinig the next url format\n",
    "\n",
    "https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10q/{`COMPANY_NAME`}_10q_{`MONTH`}_{`YEAR`}.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in COMPANIES:\n",
    "    if not os.path.exists(DATA_DIRS[c]):\n",
    "        os.mkdir(DATA_DIRS[c])\n",
    "        command = f\"wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/{c}_2021.pdf' -O '{DATA_DIRS[c]}/{c}_2021.pdf'\"        \n",
    "        subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create index and query engine for each company individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Index\n",
      "Loading Index\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# file_extractor = {\n",
    "#     \".pdf\": LlamaParse(\n",
    "#         result_type=\"markdown\",\n",
    "#         api_key=\"llx-...\",\n",
    "#     )\n",
    "# }\n",
    "\n",
    "query_engine_dict = {}\n",
    "for c in COMPANIES:\n",
    "    if not os.path.exists(PERSIST_DIRS[c]):\n",
    "        print(\"Creating Index\")\n",
    "        # load the documents and create the index\n",
    "        # documents = SimpleDirectoryReader(DATA_DIRS[c], file_extractor=file_extractor).load_data()\n",
    "        documents = SimpleDirectoryReader(DATA_DIRS[c]).load_data()\n",
    "        index = VectorStoreIndex.from_documents(documents)\n",
    "        # store it for later\n",
    "        index.storage_context.persist(persist_dir=PERSIST_DIRS[c])\n",
    "    else:\n",
    "        print(\"Loading Index\")\n",
    "        # load the existing index\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIRS[c])\n",
    "        index = load_index_from_storage(storage_context)\n",
    "    \n",
    "    query_engine_dict[c] = index.as_query_engine(similarity_top_k=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=query_engine_dict[c],\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"{c}_10k\",\n",
    "            description=(\n",
    "                f\"Provides information about {c} financials for year 2021. \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    for c in COMPANIES\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "- The tools get parsed into python-like definitions\n",
    "- The agent is prompted to generate a CoA plan\n",
    "- The function calls are parsed out of the plan and executed\n",
    "- The values in the plan are filled in\n",
    "- The agent generates a final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Available Parsed Functions ====\n",
      "def uber_10k(input: string):\n",
      "   \"\"\"Provides information about uber financials for year 2021. Use a detailed plain text question as input to the tool.\"\"\"\n",
      "    ...\n",
      "def lyft_10k(input: string):\n",
      "   \"\"\"Provides information about lyft financials for year 2021. Use a detailed plain text question as input to the tool.\"\"\"\n",
      "    ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Generated Chain of Abstraction ====\n",
      "To compare Uber's revenue growth to Lyft's in 2021, we need to obtain the revenue growth figures for both companies for that year.\n",
      "\n",
      "1. Retrieve Uber's revenue growth for 2021 by querying the Uber financial tool with a specific question about revenue growth. This can be done using the function call:\n",
      "   [FUNC uber_10k(\"What was Uber's revenue growth in 2021?\") = y1]\n",
      "\n",
      "2. Similarly, retrieve Lyft's revenue growth for 2021 by querying the Lyft financial tool with a specific question about revenue growth. This can be done using the function call:\n",
      "   [FUNC lyft_10k(\"What was Lyft's revenue growth in 2021?\") = y2]\n",
      "\n",
      "3. After obtaining both y1 and y2, compare the values to determine which company had higher revenue growth in 2021. This comparison does not require a function call but involves analyzing the outputs y1 and y2 to see which is greater.\n",
      "==== Executing uber_10k with inputs [\"What was Uber's revenue growth in 2021?\"] ====\n",
      "==== Executing lyft_10k with inputs [\"What was Lyft's revenue growth in 2021?\"] ====\n"
     ]
    }
   ],
   "source": [
    "worker = CoAAgentWorker.from_tools(\n",
    "    tools=query_engine_tools,\n",
    "    llm=Settings.llm,\n",
    "    verbose=True,\n",
    ")\n",
    "agent = AgentRunner(worker)\n",
    "response = agent.chat(\"How did Ubers revenue growth compare to Lyfts in 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2021, Uber's revenue growth was higher than Lyft's. Uber's revenue grew by 57%, increasing from $11,139 million in 2020 to $17,455 million. In contrast, Lyft's revenue increased by 36% compared to the prior year.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the prompting of Chain of Abstraction works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this actually work?\n",
    "\n",
    "So, under the hood we are prompting the LLM to first output the CoA, then we parse it and run functions, then we refine all that into a final output.\n",
    "\n",
    "First, we parse the tools into python-like function defintions by parsing `tool.metadata.fn_schema_str`, along with the tool name and description.\n",
    "\n",
    "You can find that code in the [utils](https://notebooks.githubusercontent.com/view/ipynb?browser=firefox&bypass_fastly=true&color_mode=auto&commit=7b52057b717451a801c583fae7efe4c4ad167455&device=unknown_device&docs_host=https%3A%2F%2Fdocs.github.com&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f72756e2d6c6c616d612f6c6c616d615f696e6465782f376235323035376237313734353161383031633538336661653765666534633461643136373435352f646f63732f646f63732f6578616d706c65732f6167656e742f636f615f6167656e742e6970796e62&logged_in=true&nwo=run-llama%2Fllama_index&path=docs%2Fdocs%2Fexamples%2Fagent%2Fcoa_agent.ipynb&platform=linux&repository_id=560704231&repository_type=Repository&version=125).\n",
    "\n",
    "What this looks like is we have a prompt like this:\n",
    "\n",
    "REASONING_PROMPT_TEMPALTE = \"\"\"Generate an abstract plan of reasoning using placeholders for the specific values and function calls needed.\n",
    "The placeholders should be labeled y1, y2, etc.\n",
    "Function calls should be represented as inline strings like [FUNC {{function_name}}({{input1}}, {{input2}}, ...) = {{output_placeholder}}].\n",
    "Assume someone will read the plan after the functions have been executed in order to make a final response.\n",
    "Not every question will require function calls to answer.\n",
    "If you do invoke a function, only use the available functions, do not make up functions.\n",
    "\n",
    "Example:\n",
    "-----------\n",
    "Available functions:\n",
    "\\`\\`\\`python\n",
    "def add(a: int, b: int) -> int:\n",
    "    \\\"\\\"\\\"Add two numbers together.\\\"\\\"\\\"\n",
    "    ...\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \\\"\\\"\\\"Multiply two numbers together.\\\"\\\"\\\"\n",
    "    ...\n",
    "\\`\\`\\`\n",
    "\n",
    "Question:\n",
    "Sally has 3 apples and buys 2 more. Then magically, a wizard casts a spell that multiplies the number of apples by 3. How many apples does Sally have now?\n",
    "\n",
    "Abstract plan of reasoning:\n",
    "After buying the apples, Sally has [FUNC add(3, 2) = y1] apples. Then, the wizard casts a spell to multiply the number of apples by 3, resulting in [FUNC multiply(y1, 3) = y2] apples.\n",
    "\n",
    "Your Turn:\n",
    "-----------\n",
    "Available functions:\n",
    "\\`\\`\\`python\n",
    "{functions}\n",
    "\\`\\`\\`\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Abstract plan of reasoning:\n",
    "\"\"\"\n",
    "\n",
    "This will generate the chain-of-abstraction reasoning.\n",
    "\n",
    "Then, the reasoning is parsed using the [output parser](https://notebooks.githubusercontent.com/view/ipynb?browser=firefox&bypass_fastly=true&color_mode=auto&commit=7b52057b717451a801c583fae7efe4c4ad167455&device=unknown_device&docs_host=https%3A%2F%2Fdocs.github.com&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f72756e2d6c6c616d612f6c6c616d615f696e6465782f376235323035376237313734353161383031633538336661653765666534633461643136373435352f646f63732f646f63732f6578616d706c65732f6167656e742f636f615f6167656e742e6970796e62&logged_in=true&nwo=run-llama%2Fllama_index&path=docs%2Fdocs%2Fexamples%2Fagent%2Fcoa_agent.ipynb&platform=linux&repository_id=560704231&repository_type=Repository&version=125).\n",
    "\n",
    "After calling the functions and filling in values, we give the LLM a chance to refine the response, using this prompt:\n",
    "\n",
    "REFINE_REASONING_PROMPT_TEMPALTE = \"\"\"Generate a response to a question by using a previous abstract plan of reasoning. Use the previous reasoning as context to write a response to the question.\n",
    "\n",
    "Example:\n",
    "-----------\n",
    "Question: \n",
    "Sally has 3 apples and buys 2 more. Then magically, a wizard casts a spell that multiplies the number of apples by 3. How many apples does Sally have now?\n",
    "\n",
    "Previous reasoning:\n",
    "After buying the apples, Sally has [FUNC add(3, 2) = 5] apples. Then, the wizard casts a spell to multiply the number of apples by 3, resulting in [FUNC multiply(5, 3) = 15] apples.\n",
    "\n",
    "Response:\n",
    "After the wizard casts the spell, Sally has 15 apples.\n",
    "\n",
    "Your Turn:\n",
    "-----------\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Previous reasoning:\n",
    "{prev_reasoning}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
