{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inpired by [this LlamaIndex example](https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding_adapter/)\n",
    "\n",
    "Making some changes to it with the only intention of trying ideas and learning.\n",
    "\n",
    "Notice that I am assuming you have the relevant API_KEYs as environmental variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-finetuning\n",
    "%pip install llama-index-embeddings-openai\n",
    "%pip install llama-index-embeddings-adapter\n",
    "%pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<jemalloc>: Unsupported system page size\n"
     ]
    }
   ],
   "source": [
    "from bubls.utils.data.download import download_file_from_url\n",
    "from bubls.utils.data.load import load_corpus\n",
    "from bubls.utils.evaluation.evaluate_embeddings import (\n",
    "    get_query_hit_pairs, sentence_transformer_ir_evaluator\n",
    ")\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.finetuning import EmbeddingAdapterFinetuneEngine\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "import os\n",
    "\n",
    "# from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.adapter.utils import TwoLayerNN\n",
    "from llama_index.embeddings.adapter import LinearAdapterEmbeddingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA = {\n",
    "    \"train\": {\n",
    "        \"lyft_10k\": {\n",
    "            \"source_url\": \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf\",\n",
    "            \"file_name\": \"lyft_10k_2021.pdf\",\n",
    "            \"save_data_to\": os.path.join(os.environ[\"DATA_DIR\"], \"lyft_10k\"),\n",
    "        }\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"uber_10k\": {\n",
    "            \"source_url\": \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\",\n",
    "            \"file_name\": \"uber_10k_2021.pdf\",\n",
    "            \"save_data_to\": os.path.join(os.environ[\"DATA_DIR\"], \"uber_10k\"),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "PERSIST_FINETUNE_DATA_TO = os.path.join(os.environ[\"PERSIST_DIR\"], \"eg1_finetune_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data\n",
    "- Download Information\n",
    "- Split train and validation data\n",
    "- Load corpus\n",
    "- Generate QA embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data with QA embedding pairs\n",
      "Loading data with QA embedding pairs\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for split in METADATA:\n",
    "    files = []\n",
    "    for k, md in METADATA[split].items():\n",
    "        files.append(\n",
    "            download_file_from_url(md[\"source_url\"], md[\"file_name\"], md[\"save_data_to\"])\n",
    "        )\n",
    "    data_path = os.path.join(PERSIST_FINETUNE_DATA_TO, f\"{split}_data.json\")\n",
    "    if not os.path.exists(data_path):\n",
    "        if not os.path.exists(PERSIST_FINETUNE_DATA_TO):\n",
    "            os.makedirs(PERSIST_FINETUNE_DATA_TO)\n",
    "        print(\"Generating data with QA embedding pairs\")\n",
    "        # For every node we have id, embedding placeholder, metadata, text, relationships, etc.\n",
    "        nodes = load_corpus(files, pct_sample = 0.2)\n",
    "        data[split] = generate_qa_embedding_pairs(\n",
    "            llm=OpenAI(model=\"gpt-3.5-turbo\"), nodes=nodes\n",
    "        )\n",
    "        data[split].save_json(data_path)\n",
    "    else:\n",
    "        print(\"Loading data with QA embedding pairs\")\n",
    "        data[split] = EmbeddingQAFinetuneDataset.from_json(data_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbedding()\n",
    "# embedding_model = resolve_embed_model(\"local:BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define two-layer Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_model = TwoLayerNN(\n",
    "    1536,  # input dimension. Change depending on embedding model\n",
    "    1024,  # hidden dimension\n",
    "    1536,  # output dimension Change depending on embedding model\n",
    "    bias=True,\n",
    "    add_residual=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine = EmbeddingAdapterFinetuneEngine(\n",
    "    data[\"train\"],\n",
    "    embedding_model,\n",
    "    model_output_path=\"adapter_test3\",\n",
    "    model_checkpoint_path=\"adapter_ck3\",\n",
    "    adapter_model=adapter_model,\n",
    "    batch_size=4,\n",
    "    epochs=4,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "finetune_engine.finetune()\n",
    "embed_model = finetune_engine.get_finetuned_model(adapter_cls=TwoLayerNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalute and Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI, no fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9142857142857143"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_embedding = OpenAIEmbedding()\n",
    "ada_val_results = get_query_hit_pairs(data[\"val\"], openai_embedding)\n",
    "ada_val_results[\"is_hit\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9316374755857143"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_val_results = get_query_hit_pairs(data[\"val\"], embed_model)\n",
    "fine_tuned_val_results[\"is_hit\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
