{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inpired by [this LlamaIndex example](https://github.com/run-llama/finetune-embedding/blob/main/finetune.ipynb)\n",
    "\n",
    "Making some changes to it with the only intention of trying ideas and learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-finetuning\n",
    "%pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bubls.utils.data.download import download_file_from_url\n",
    "from bubls.utils.data.load import load_corpus\n",
    "from bubls.utils.evaluation.evaluate_embeddings import get_query_hit_pairs\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    losses\n",
    ")\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from sentence_transformers import InputExample\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA = {\n",
    "    \"train\": {\n",
    "        \"lyft_10k\": {\n",
    "            \"source_url\": \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf\",\n",
    "            \"file_name\": \"lyft_10k_2021.pdf\",\n",
    "            \"save_data_to\": os.path.join(os.environ[\"DATA_DIR\"], \"lyft_10k\"),\n",
    "        }\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"uber_10k\": {\n",
    "            \"source_url\": \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\",\n",
    "            \"file_name\": \"uber_10k_2021.pdf\",\n",
    "            \"save_data_to\": os.path.join(os.environ[\"DATA_DIR\"], \"uber_10k\"),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "PERSIST_FINETUNE_DATA_TO = os.path.join(os.environ[\"PERSIST_DIR\"], \"naive_finetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data\n",
    "- Download Information\n",
    "- Split train and validation data\n",
    "- Load corpus\n",
    "- Generate QA embeddings\n",
    "\n",
    "This is a very naive example and we are loading a very small sample of a file (pct_sample). Only intention of this was to run it fast and understand how the code works without the intention of training an actual good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data with QA embedding pairs\n",
      "Loading data with QA embedding pairs\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for split in METADATA:\n",
    "    files = []\n",
    "    for k, md in METADATA[split].items():\n",
    "        files.append(\n",
    "            download_file_from_url(md[\"source_url\"], md[\"file_name\"], md[\"save_data_to\"])\n",
    "        )\n",
    "    data_path = os.path.join(PERSIST_FINETUNE_DATA_TO, f\"{split}_data.json\")\n",
    "    if not os.path.exists(data_path):\n",
    "        if not os.path.exists(PERSIST_FINETUNE_DATA_TO):\n",
    "            os.mkdir(PERSIST_FINETUNE_DATA_TO)\n",
    "        print(\"Generating data with QA embedding pairs\")\n",
    "        # For every node we have id, embedding placeholder, metadata, text, relationships, etc.\n",
    "        nodes = load_corpus(files, pct_sample = 0.2)\n",
    "        data[split] = generate_qa_embedding_pairs(\n",
    "            llm=OpenAI(model=\"gpt-3.5-turbo\"), nodes=nodes\n",
    "        )\n",
    "        data[split].save_json(data_path)\n",
    "    else:\n",
    "        print(\"Loading data with QA embedding pairs\")\n",
    "        data[split] = EmbeddingQAFinetuneDataset.from_json(data_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Loader and Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = []\n",
    "for query_id, query in data[\"train\"].queries.items():\n",
    "    node_id = data[\"train\"].relevant_docs[query_id][0]\n",
    "    text = data[\"train\"].corpus[node_id]\n",
    "    example = InputExample(texts=[query, text])\n",
    "    training_examples.append(example)\n",
    "\n",
    "loader = DataLoader(training_examples, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = InformationRetrievalEvaluator(\n",
    "    data[\"val\"].queries,\n",
    "    data[\"val\"].corpus,\n",
    "    data[\"val\"].relevant_docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"BAAI/bge-small-en\"\n",
    "embedding_model = SentenceTransformer(model_id)\n",
    "loss = losses.MultipleNegativesRankingLoss(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "warmup_steps = int(len(loader) * epochs * 0.1)\n",
    "\n",
    "embedding_model.fit(\n",
    "    train_objectives=[(loader, loss)],\n",
    "    epochs=1,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='exp_finetune',\n",
    "    show_progress_bar=True,\n",
    "    evaluator=evaluator, \n",
    "    evaluation_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6567901234567901"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = get_query_hit_pairs(\n",
    "    data[\"val\"],\n",
    "    embedding_model,\n",
    "    top_k = 5,\n",
    "    verbose = False,\n",
    ")\n",
    "eval_df[\"is_hit\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
