{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inpired by [this LlamaIndex example](https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding_adapter/)\n",
    "\n",
    "Making some changes to it with the only intention of trying ideas and learning.\n",
    "\n",
    "Notice that I am assuming you have the relevant API_KEYs as environmental variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.7.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.10-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.8-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Using cached thinc-8.2.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.4.8-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.11/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Using cached blis-0.7.11-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Using cached confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached marisa-trie-1.1.0.tar.gz (410 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hUsing cached spacy-3.7.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (6.2 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.8-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (44 kB)\n",
      "Using cached langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.10-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (29 kB)\n",
      "Using cached preshed-3.0.9-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (150 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.4.8-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (487 kB)\n",
      "Using cached thinc-8.2.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (868 kB)\n",
      "Using cached typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "Using cached blis-0.7.11-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.7 MB)\n",
      "Using cached cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "Using cached confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Building wheels for collected packages: marisa-trie\n",
      "  Building wheel for marisa-trie (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_clib\n",
      "  \u001b[31m   \u001b[0m building 'libmarisa-trie' library\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-cpython-311/marisa-trie\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-cpython-311/marisa-trie/lib\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-cpython-311/marisa-trie/lib/marisa\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-cpython-311/marisa-trie/lib/marisa/grimoire\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-cpython-311/marisa-trie/lib/marisa/grimoire/io\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-cpython-311/marisa-trie/lib/marisa/grimoire/trie\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-cpython-311/marisa-trie/lib/marisa/grimoire/vector\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/compiler_compat -Wsign-compare -DNDEBUG -fwrapv -O3 -Wall -fPIC -O3 -isystem /opt/conda/include -fPIC -O3 -isystem /opt/conda/include -fPIC -Imarisa-trie/lib -Imarisa-trie/include -c marisa-trie/lib/marisa/agent.cc -o build/temp.linux-aarch64-cpython-311/marisa-trie/lib/marisa/agent.o\n",
      "  \u001b[31m   \u001b[0m error: command 'gcc' failed: No such file or directory\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for marisa-trie\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for marisa-trie\n",
      "Failed to build marisa-trie\n",
      "\u001b[31mERROR: Could not build wheels for marisa-trie, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install llama-index-finetuning\n",
    "# %pip install llama-index-finetuning-callbacks\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index pypdf sentence-transformers ragas\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<jemalloc>: Unsupported system page size\n"
     ]
    }
   ],
   "source": [
    "from bubls.utils.data.download import download_file_from_url\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "import random\n",
    "# from bubls.utils.data.load import load_corpus\n",
    "# from bubls.utils.evaluation.evaluate_embeddings import (\n",
    "#     get_query_hit_pairs, sentence_transformer_ir_evaluator\n",
    "# )\n",
    "# from bubls.utils.custom_models.embedding_adapter import CustomNN\n",
    "# from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "# from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "# from llama_index.finetuning import EmbeddingAdapterFinetuneEngine\n",
    "# from llama_index.core.embeddings import resolve_embed_model\n",
    "# import torch\n",
    "import os\n",
    "\n",
    "# # from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "# from llama_index.embeddings.adapter.utils import TwoLayerNN\n",
    "# from llama_index.embeddings.adapter import LinearAdapterEmbeddingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA = {\n",
    "    \"train\": {\n",
    "        \"ipcc_ch3\": {\n",
    "            \"source_url\": \"https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf\",\n",
    "            \"file_name\": \"IPCC_AR6_WGII_Chapter03.pdf\",\n",
    "            \"save_data_to\": os.path.join(os.environ[\"DATA_DIR\"], \"ipcc_ch3\"),\n",
    "        }\n",
    "    },\n",
    "    # \"val\": {\n",
    "    #     \"uber_10k\": {\n",
    "    #         \"source_url\": \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\",\n",
    "    #         \"file_name\": \"uber_10k_2021.pdf\",\n",
    "    #         \"save_data_to\": os.path.join(os.environ[\"DATA_DIR\"], \"uber_10k\"),\n",
    "    #     }\n",
    "    # }\n",
    "}\n",
    "\n",
    "PERSIST_FINETUNE_DATA_TO = os.path.join(os.environ[\"PERSIST_DIR\"], \"eg1_finetune_llm\")\n",
    "\n",
    "QUESTION_GEN_QUERY = (\n",
    "    \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "    \"a quiz/examination. Using the provided context, formulate \"\n",
    "    \"a single question that captures an important fact from the \"\n",
    "    \"context. Restrict the question to the context information provided.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data\n",
    "- Download Information\n",
    "- Use simple directory reader to load data\n",
    "- Use DatasetGenerator and gpt3.5 to generate a dataset based on a query request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Spacy is not installed, please install it with `pip install spacy`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/llama_index/core/postprocessor/node.py:38\u001b[0m, in \u001b[0;36mKeywordNodePostprocessor._postprocess_nodes\u001b[0;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m documents[split] \u001b[38;5;241m=\u001b[39m SimpleDirectoryReader(input_files\u001b[38;5;241m=\u001b[39mfiles)\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m     12\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(documents[split])\n\u001b[0;32m---> 14\u001b[0m dataset_generator \u001b[38;5;241m=\u001b[39m DatasetGenerator\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m     15\u001b[0m     documents[split][:\u001b[38;5;241m50\u001b[39m],\n\u001b[1;32m     16\u001b[0m     question_gen_query\u001b[38;5;241m=\u001b[39mQUESTION_GEN_QUERY,\n\u001b[1;32m     17\u001b[0m     llm\u001b[38;5;241m=\u001b[39mOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m),\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m questions \u001b[38;5;241m=\u001b[39m dataset_generator\u001b[38;5;241m.\u001b[39mgenerate_questions_from_nodes(num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:209\u001b[0m, in \u001b[0;36mDatasetGenerator.from_documents\u001b[0;34m(cls, documents, llm, transformations, callback_manager, num_questions_per_chunk, text_question_template, text_qa_template, question_gen_query, required_keywords, exclude_keywords, show_progress, service_context)\u001b[0m\n\u001b[1;32m    203\u001b[0m node_postprocessor \u001b[38;5;241m=\u001b[39m KeywordNodePostprocessor(\n\u001b[1;32m    204\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[1;32m    205\u001b[0m     required_keywords\u001b[38;5;241m=\u001b[39mrequired_keywords,\n\u001b[1;32m    206\u001b[0m     exclude_keywords\u001b[38;5;241m=\u001b[39mexclude_keywords,\n\u001b[1;32m    207\u001b[0m )\n\u001b[1;32m    208\u001b[0m node_with_scores \u001b[38;5;241m=\u001b[39m [NodeWithScore(node\u001b[38;5;241m=\u001b[39mnode) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[0;32m--> 209\u001b[0m node_with_scores \u001b[38;5;241m=\u001b[39m node_postprocessor\u001b[38;5;241m.\u001b[39mpostprocess_nodes(node_with_scores)\n\u001b[1;32m    210\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [node_with_score\u001b[38;5;241m.\u001b[39mnode \u001b[38;5;28;01mfor\u001b[39;00m node_with_score \u001b[38;5;129;01min\u001b[39;00m node_with_scores]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    213\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m    214\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[1;32m    222\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/llama_index/core/postprocessor/types.py:55\u001b[0m, in \u001b[0;36mBaseNodePostprocessor.postprocess_nodes\u001b[0;34m(self, nodes, query_bundle, query_str)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_nodes(nodes, query_bundle)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/llama_index/core/postprocessor/node.py:40\u001b[0m, in \u001b[0;36mKeywordNodePostprocessor._postprocess_nodes\u001b[0;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpacy is not installed, please install it with `pip install spacy`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PhraseMatcher\n\u001b[1;32m     45\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mblank(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang)\n",
      "\u001b[0;31mImportError\u001b[0m: Spacy is not installed, please install it with `pip install spacy`."
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "documents = {}\n",
    "for split in METADATA:\n",
    "    files = []\n",
    "    for k, md in METADATA[split].items():\n",
    "        files.append(\n",
    "            download_file_from_url(md[\"source_url\"], md[\"file_name\"], md[\"save_data_to\"])\n",
    "        )\n",
    "    data_path = os.path.join(PERSIST_FINETUNE_DATA_TO, f\"{split}_data.txt\")\n",
    "    if not os.path.exists(data_path):\n",
    "        documents[split] = SimpleDirectoryReader(input_files=files).load_data()\n",
    "        random.shuffle(documents[split])\n",
    "\n",
    "        dataset_generator = DatasetGenerator.from_documents(\n",
    "            documents[split][:50],\n",
    "            question_gen_query=QUESTION_GEN_QUERY,\n",
    "            llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3),\n",
    "        )\n",
    "\n",
    "        questions = dataset_generator.generate_questions_from_nodes(num=2)\n",
    "        with open(data_path, \"w\") as f:\n",
    "            for question in questions:\n",
    "                f.write(question + \"\\n\")\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
