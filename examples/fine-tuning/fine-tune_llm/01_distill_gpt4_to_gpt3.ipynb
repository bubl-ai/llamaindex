{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inpired by [this LlamaIndex example](https://colab.research.google.com/drive/1NgyCJVyrC2xcZ5lxt2frTU862v6eJHlc?usp=sharing#scrollTo=QOAAE83mPwYd)\n",
    "\n",
    "Making some changes to it with the only intention of trying ideas and learning.\n",
    "\n",
    "Notice that I am assuming you have the relevant API_KEYs as environmental variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-finetuning\n",
    "# %pip install llama-index-finetuning-callbacks\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index pypdf sentence-transformers ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bubls.utils.data.download import download_file_from_url\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    Settings,\n",
    "    ServiceContext\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.finetuning.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.finetuning import OpenAIFinetuneEngine\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA = {\n",
    "    \"questions\": {\n",
    "        \"ipcc_ch3\": {\n",
    "            \"source_url\": \"https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf\",\n",
    "            \"file_name\": \"IPCC_AR6_WGII_Chapter03.pdf\",\n",
    "            \"save_data_to\": os.path.join(os.environ[\"DATA_DIR\"], \"ipcc_ch3\"),\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "PERSIST_FINETUNE_DATA_TO = os.path.join(os.environ[\"PERSIST_DIR\"], \"eg1_finetune_llm\")\n",
    "\n",
    "QUESTION_GEN_QUERY = (\n",
    "    \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "    \"a quiz/examination. Using the provided context, formulate \"\n",
    "    \"a single question that captures an important fact from the \"\n",
    "    \"context. Restrict the question to the context information provided.\"\n",
    ")\n",
    "\n",
    "# limit the context window to test refine process\n",
    "Settings.context_window = 2048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data\n",
    "- Download Information\n",
    "- Use simple directory reader to load data\n",
    "- Create a baseline query_engine with gpt3.5 from index based on documents\n",
    "- Use DatasetGenerator to generate a dataset based on a query request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index\n",
      "Loading questions\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "documents = {}\n",
    "query_engine_dict = {}\n",
    "for split in METADATA:\n",
    "    files = []\n",
    "    for k, md in METADATA[split].items():\n",
    "        files.append(\n",
    "            download_file_from_url(md[\"source_url\"], md[\"file_name\"], md[\"save_data_to\"])\n",
    "        )\n",
    "    data_path = os.path.join(PERSIST_FINETUNE_DATA_TO, f\"{split}.txt\")\n",
    "    index_path = os.path.join(PERSIST_FINETUNE_DATA_TO, \"index\")\n",
    "    documents[split] = SimpleDirectoryReader(input_files=files).load_data()\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"Generating index\")\n",
    "        index = VectorStoreIndex.from_documents(documents[split])\n",
    "        index.storage_context.persist(persist_dir=index_path)\n",
    "        print(\"Generating questions\")\n",
    "        os.makedirs(PERSIST_FINETUNE_DATA_TO, exist_ok=True)\n",
    "        random.shuffle(documents[split])\n",
    "        dataset_generator = DatasetGenerator.from_documents(\n",
    "            documents[split][:50],\n",
    "            question_gen_query=QUESTION_GEN_QUERY,\n",
    "            llm=gpt_35_llm,\n",
    "        )\n",
    "\n",
    "        questions = dataset_generator.generate_questions_from_nodes(num=10)\n",
    "        with open(data_path, \"w\") as f:\n",
    "            for question in questions:\n",
    "                f.write(question + \"\\n\")\n",
    "    else:\n",
    "        print(\"Loading index\")\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=index_path)\n",
    "        index = load_index_from_storage(storage_context)\n",
    "        print(\"Loading questions\")\n",
    "        questions = []\n",
    "        with open(data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                questions.append(line.strip())\n",
    "query_engine_dict[f\"baseline\"] = index.as_query_engine(similarity_top_k=2, llm=gpt_35_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate baseline: gpt3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.8356, 'answer_relevancy': 0.9725, 'faithfulness': 0.7325}\n"
     ]
    }
   ],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine_dict[\"baseline\"].query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GPT4 to collect training data to distill into 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17629/4290419199.py:4: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  gpt_4_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "# OpenAIFineTuningHandler callback automatically logs questions/answers to a dataset\n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-4\", temperature=0.3),\n",
    "    context_window=Settings.context_window,\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents[\"questions\"], service_context=gpt_4_context)\n",
    "query_engine_dict[\"gpt-4\"] = index.as_query_engine(similarity_top_k=2)\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine_dict[\"gpt-4\"].query(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_handler.save_finetuning_events(\"finetuning_events.jsonl\")\n",
    "finetune_engine = OpenAIFinetuneEngine(\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"finetuning_events.jsonl\",\n",
    "    # start_job_id=\"<start-job-id>\"  # use if you have an existing job\n",
    ")\n",
    "\n",
    "# finetune_engine = OpenAIFinetuneEngine.from_finetuning_handler(\n",
    "#     finetuning_handler,\n",
    "#     \"gpt-3.5-turbo\",\n",
    "#     \"tmp.jsonl\"\n",
    "# )\n",
    "\n",
    "finetune_engine.finetune()\n",
    "ft_llm = finetune_engine.get_finetuned_model(temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune_engine.get_current_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_context = ServiceContext.from_defaults(\n",
    "    llm=ft_llm,\n",
    "    context_window=Settings.context_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.8680, 'answer_relevancy': 0.9607, 'faithfulness': 0.7917}\n"
     ]
    }
   ],
   "source": [
    "contexts_ft = []\n",
    "answers_ft = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine_dict[\"baseline\"].query(question)\n",
    "    contexts_ft.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers_ft.append(str(response))\n",
    "\n",
    "ds_ft = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers_ft,\n",
    "        \"contexts\": contexts_ft,\n",
    "    }\n",
    ")\n",
    "\n",
    "result_ft = evaluate(ds_ft, [answer_relevancy, faithfulness])\n",
    "print(result_ft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
